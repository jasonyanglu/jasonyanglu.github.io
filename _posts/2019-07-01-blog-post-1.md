---
title: 'Deep Reinforcement Learning: Pong from Pixels学习笔记'
date: 2019-07-01
permalink: /posts/2019/07/blog-post-1/
tags:
  - 强化学习
  - 学习笔记
---

Deep Reinforcement Learning: Pong from Pixels是一篇很好的强化学习入门博文。其中详细介绍了DRL一种常用的方法：Policy Gradient。原文链接：http://karpathy.github.io/2016/05/31/rl/



# 乒乓球游戏

乒乓球游戏是一个最简单的也是最好的例子来说明如何强化学习方法是如何工作的。这个游戏玩家可以通过上下移动球拍来接球，成功接到球+1分，没接到-1分。

![](http://karpathy.github.io/assets/rl/pong.gif)

对于我们的算法来说，这个游戏的输入就是每一帧的全局像素（210\*160\*3），输出则是向上移动还是向下移动球拍。

## 策略网络

策略网络就是一个普通的前馈神经网络，输入是把像素矩阵拉平之后的向量，通过权重W1，W2，以及激活函数，最终输出行为的概率，例如向上40%，向下60%。得到行为的概率之后，通过采样决定下一步采用哪个行为，之后再通过这个行为的reward来更新策略网络的参数。代码很简单：

```python
h = np.dot(W1, x) # compute hidden layer neuron activations
h[h<0] = 0 # ReLU nonlinearity: threshold at zero
logp = np.dot(W2, h) # compute log probability of going up
p = 1.0 / (1.0 + np.exp(-logp)) # sigmoid function (gives probability of going up)
```

一句话描述Policy gradient: 使用一种policy跑一会儿，然后看看这种policy下的哪种行为会得到高分，然偶增加这种行为的概率。

## 训练模型

将W1，W2随机初始化，进行100次乒乓球游戏。假设每次游戏有200帧，那么就将得到2万次行为决策。假如最终我们赢了12次，输了88次，2万次里面的2400次决策就会使用正反馈对网络进行更新，另外17600次决策就会用负反馈对网络进行更新。更新之后如果再进行100次游戏，则决策就会向赢的概率大的方向移动。

## 与有监督学习的关系

其实PG与有监督学习非常类似，只有2个细微的差别。

1. 有监督学习在得到logit并softmax之后，可以通过和真实标签计算交叉熵得到loss再计算梯度以及使用反向传播更新神经网络。然而在PG的学习过程中，我们并没有真实标签y，softmax之后得到的是产生每个action的概率，例如向上移动0.7，向下移动0.3。由于没有标签，只能通过对action的概率进行采样，采样到的action就是它的标签，所以PG的标签可以称为“假标签”。
2. 采样得到的action应用到环境中，然后通过环境给出的reward来计算梯度。所以PG的loss计算其实就是$A_i\log p(y_i\rvert x_i)$，$A_i$是在当前环境$x_i$下通过policy $p(x)$采样得到的行为$y_i$。因为一个step只能有一个action，所以softmax的其他项都是0（我们不可能在当前step计算每个action的$A_i$）。

## 折扣Reward

一个回合中，每个step都希望考虑除了本身的reward，还加上一些未来的reward，但是离当前step越远的reward的权重就越低。这样做使得早期的action与最终一个episode的结局reward有了联系，尤其是在一些场景中，每个step没有即时的reward，只有当一个episode结束后才能知道这一系列action将得到多少reward。


<center>$R_t=\sum_{k=0}^\inf \gamma^kr_{t+k}=r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + ...$
</center>

$$R_t=\sum_{k=0}^\inf \gamma^kr_{t+k}=r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} +...$$

## PG的数学推导

PG的loss，或者说是最终得分其实很简单，就是$E_{x\sim p(x\rvert\theta)}[f(x)]$，就是在当前policy $p(x)$下的得分$f(x)$的期望。所以我们需要计算的就是得分期望的梯度$\nabla_\theta E_{x\sim p(x\rvert\theta)}[f(x)]$。推导如下：

![](https://jasonyanglu.github.io/images/2019-07-01-blog-post-1.assets/image-20190701112717103.png)

其实本质上就是如何将求梯度从期望外移进去，并且再写成期望的形式的一个过程。从最后公式推导最后一行可以看出，最终的形式是每一步的概率求log之后计算梯度，再乘上对应的得分，最后再求均值。

# 一些思考

对于人类来说，你只要跟他说清楚这个游戏的规则，那么他大概就可以上手玩了，并不需要学习。人类和RL学习玩游戏有几个不同或相同之处：

1. RL不接受任何规则输入，完全凭空通过试错去猜测游戏的模式。从这个角度来看，如果不跟人类说游戏规则，让人通过试错去猜测游戏玩法，那么RL会比人强很多。尤其是如果把屏幕上的像素位置全部打乱，人类根本无法通过视觉学会游戏规则，而RL可以轻易通过全连接的神经网络学会。
2. 人类带有大量的先验知识。例如球会反弹，弹出的角度与弹入的角度是一样的，方向键对应着球拍的移动，等等。然而RL什么都不知道，全凭摸索。
3. PG其实是一种暴力搜索的方法，尝试每个动作，然后把好的动作记下来。然而人类在学习新东西的时候，也会潜意识地将正确的行为变成肌肉记忆。例如骑车或游泳，一开始的时候会遍做遍想该怎么做，熟练（不停试错并且有正反馈）之后，则不需要思考，单靠肌肉记忆就可以完成。
4. 试错是RL学习的必要条件，然而不是人类学习的必要条件。还是由于先验知识，人类可以在不经历任何反馈的情况下，做出正确的行为。例如，人类不需要真实经历撞车几百次，才能学会如何避免撞车。先验知识决定了人类可以迅速将游戏规则（策略网络）高度抽象化（用一句话来描述游戏规则）。






